apiVersion: v1
data:
  condorHostConfig: |+
    {
            "aipanda083.cern.ch": {
                    "pool": "aipanda083.cern.ch:19618",
                    "weight": 1
            }
    }

  runpilot2-wrapper.sh: "#!/bin/bash\n#\n# pilot2 wrapper used at CERN central pilot
    factories\n#\n# https://google.github.io/styleguide/shell.xml\n\nVERSION=20200331a-pilot2\n\nfunction
    err() {\n  dt=$(date --utc +\"%Y-%m-%d %H:%M:%S,%3N [wrapper]\")\n  echo $dt $@
    >&2\n}\n\nfunction log() {\n  dt=$(date --utc +\"%Y-%m-%d %H:%M:%S,%3N [wrapper]\")\n
    \ echo $dt $@\n}\n\nfunction get_workdir {\n  if [[ ${piloturl} == 'local' &&
    ${harvesterflag} == 'false' ]]; then\n    echo $(pwd)\n    return 0\n  fi\n\n
    \ if [[ ${harvesterflag} == 'true' ]]; then\n    # test if Harvester WorkFlow
    is OneToMany aka \"Jumbo\" Jobs\n    if [[ ${workflowarg} == 'OneToMany' ]]; then\n
    \     if [[ -n ${!harvesterarg} ]]; then\n        templ=$(pwd)/atlas_${!harvesterarg}\n
    \       mkdir ${templ}\n        echo ${templ}\n        return 0\n      fi\n    else\n
    \     echo $(pwd)\n      return 0\n    fi\n  fi\n\n  if [[ -n ${OSG_WN_TMP} ]];
    then\n    templ=${OSG_WN_TMP}/atlas_XXXXXXXX\n  elif [[ -n ${TMPDIR} ]]; then\n
    \   templ=${TMPDIR}/atlas_XXXXXXXX\n  else\n    templ=$(pwd)/atlas_XXXXXXXX\n
    \ fi\n  temp=$(mktemp -d $templ)\n  echo ${temp}\n}\n\n\nfunction check_python()
    {\n  pybin=$(which python)\n  if [[ $? -ne 0 ]]; then\n    log \"FATAL: python
    not found in PATH\"\n    err \"FATAL: python not found in PATH\"\n    if [[ -z
    \"${PATH}\" ]]; then\n      log \"In fact, PATH env var is unset mon amie\"\n
    \     err \"In fact, PATH env var is unset mon amie\"\n    fi\n    log \"PATH
    content is ${PATH}\"\n    err \"PATH content is ${PATH}\"\n    apfmon_fault 1\n
    \   sortie 1\n  fi\n    \n  pyver=$($pybin -c \"import sys; print '%03d%03d%03d'
    % sys.version_info[0:3]\")\n  # check if native python version > 2.6.0\n  if [[
    ${pyver} -ge 002006000 ]] ; then\n    log \"Native python version is > 2.6.0 (${pyver})\"\n
    \   log \"Using ${pybin} for python compatibility\"\n  else\n    log \"ERROR:
    this site has native python < 2.6.0\"\n    err \"ERROR: this site has native python
    < 2.6.0\"\n    log \"Native python ${pybin} is old: ${pyver}\"\n  \n    # Oh dear,
    we're doomed...\n    log \"FATAL: Failed to find a compatible python, exiting\"\n
    \   err \"FATAL: Failed to find a compatible python, exiting\"\n    apfmon_fault
    1\n    sortie 1\n  fi\n}\n\nfunction check_proxy() {\n  voms-proxy-info -all\n
    \ if [[ $? -ne 0 ]]; then\n    log \"WARNING: error running: voms-proxy-info -all\"\n
    \   err \"WARNING: error running: voms-proxy-info -all\"\n    arcproxy -I\n    if
    [[ $? -eq 127 ]]; then\n      log \"FATAL: error running: arcproxy -I\"\n      err
    \"FATAL: error running: arcproxy -I\"\n      apfmon_fault 1\n      sortie 1\n
    \   fi\n  fi\n}\n\nfunction check_cvmfs() {\n  export VO_ATLAS_SW_DIR=${VO_ATLAS_SW_DIR:-/cvmfs/atlas.cern.ch/repo/sw}\n
    \ if [[ -d ${VO_ATLAS_SW_DIR} ]]; then\n    log \"Found atlas software repository\"\n
    \ else\n    log \"ERROR: atlas software repository NOT found: ${VO_ATLAS_SW_DIR}\"\n
    \   log \"FATAL: Failed to find atlas software repository\"\n    err \"FATAL:
    Failed to find atlas software repository\"\n    apfmon_fault 1\n    sortie 1\n
    \ fi\n}\n  \nfunction setup_alrb() {\n  log 'NOTE: rucio,davix,xrootd setup now
    done in local site setup atlasLocalSetup.sh'\n  if [[ ${iarg} == \"RC\" ]]; then\n
    \   log 'RC pilot requested, setting ALRB_rucioVersion=testing'\n    export ALRB_rucioVersion=testing\n
    \ fi\n  if [[ ${iarg} == \"ALRB\" ]]; then\n    log 'ALRB pilot requested, setting
    ALRB env vars to testing'\n    export ALRB_adcTesting=YES\n  fi\n  export ATLAS_LOCAL_ROOT_BASE=${ATLAS_LOCAL_ROOT_BASE:-/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase}\n
    \ export ALRB_userMenuFmtSkip=YES\n  export ALRB_noGridMW=${ALRB_noGridMW:-NO}\n\n
    \ if [[ ${ALRB_noGridMW} == \"YES\" ]]; then\n    log \"Site has set ALRB_noGridMW=YES
    so use site native install rather than ALRB\"\n    if [[ ${tflag} == 'true' ]];
    then\n      log 'Skipping proxy checks due to -t flag'\n    else\n      check_vomsproxyinfo
    || check_arcproxy\n      if [[ $? -eq 1 ]]; then\n        log \"FATAL: Site MW
    being used but proxy tools not found\"\n        err \"FATAL: Site MW being used
    but proxy tools not found\"\n        apfmon_fault 1\n        sortie 1\n      fi\n
    \   fi\n  else\n    log \"Will use ALRB MW because ALRB_noGridMW=NO (default)\"\n
    \ fi\n\n  if [ -d ${ATLAS_LOCAL_ROOT_BASE} ]; then\n    log 'source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh
    --quiet'\n    source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh --quiet\n
    \ else\n    log \"ERROR: ALRB ATLAS_LOCAL_ROOT_BASE not found: ${ATLAS_LOCAL_ROOT_BASE},
    exiting\"\n    err \"ERROR: ALRB ATLAS_LOCAL_ROOT_BASE not found: ${ATLAS_LOCAL_ROOT_BASE},
    exiting\"\n    apfmon_fault 1\n    sortie 1\n  fi\n}\n\nfunction setup_local()
    {\n  log \"Looking for ${VO_ATLAS_SW_DIR}/local/setup.sh\"\n  if [[ -f ${VO_ATLAS_SW_DIR}/local/setup.sh
    ]]; then\n    log \"Sourcing ${VO_ATLAS_SW_DIR}/local/setup.sh -s ${qarg}\"\n
    \   source ${VO_ATLAS_SW_DIR}/local/setup.sh -s ${qarg}\n  else\n    log 'WARNING:
    No ATLAS local setup found'\n    err 'WARNING: this site has no local setup ${VO_ATLAS_SW_DIR}/local/setup.sh'\n
    \ fi\n  # OSG MW setup\n  if [[ -f ${OSG_GRID}/setup.sh ]]; then\n    log \"Setting
    up OSG MW using ${OSG_GRID}/setup.sh\"\n    source ${OSG_GRID}/setup.sh\n  fi\n}\n\nfunction
    setup_shoal() {\n  log \"will set FRONTIER_SERVER with shoal\"\n  if [[ -n \"${FRONTIER_SERVER}\"
    ]] ; then\n    export FRONTIER_SERVER\n    log \"call shoal frontier\"\n    outputstr=`shoal-client
    -f`\n    log \"result: $outputstr\"\n\n    if [[ $? -eq 0 ]] ; then\n      export
    FRONTIER_SERVER=$outputstr\n    fi\n\n    log \"set FRONTIER_SERVER = $FRONTIER_SERVER\"\n
    \ fi\n}\n\nfunction setup_harvester_symlinks() {\n  for datafile in `find ${HARVESTER_WORKDIR}
    -maxdepth 1 -type l -exec /usr/bin/readlink -e {} ';'`; do\n      symlinkname=$(basename
    $datafile)\n      ln -s $datafile $symlinkname\n  done      \n}\n\n\nfunction
    check_vomsproxyinfo() {\n  out=$(voms-proxy-info --version 2>/dev/null)\n  if
    [[ $? -eq 0 ]]; then\n    log \"Check version: ${out}\"\n    return 0\n  else\n
    \   log \"voms-proxy-info not found\"\n    return 1\n  fi\n}\n\nfunction check_arcproxy()
    {\n  out=$(arcproxy --version 2>/dev/null)\n  if [[ $? -eq 0 ]]; then\n    log
    \"Check version: ${out}\"\n    return 0\n  else\n    log \"arcproxy not found\"\n
    \   return 1\n  fi\n}\n\nfunction pilot_cmd() {\n\n  # test if not harvester job
    \n  if [[ ${harvesterflag} == 'false' ]] ; then  \n    if [[ -n ${pilotversion}
    ]]; then\n      cmd=\"${pybin} pilot2/pilot.py -q ${qarg} -i ${iarg} -j ${jarg}
    --pilot-user=ATLAS ${pilotargs}\"\n    else\n      cmd=\"${pybin} pilot2/pilot.py
    -q ${qarg} -i ${iarg} -j ${jarg} --pilot-user=ATLAS ${pilotargs}\"\n    fi\n  else\n
    \   # check to see if we are running OneToMany Harvester workflow (aka Jumbo Jobs)\n
    \   if [[ ${workflowarg} == 'OneToMany' ]] && [ -z ${HARVESTER_PILOT_WORKDIR+x}
    ] ; then\n      cmd=\"${pybin} pilot2/pilot.py -q ${qarg} -i ${iarg} -j ${jarg}
    -a ${HARVESTER_PILOT_WORKDIR} --pilot-user=ATLAS ${pilotargs}\"\n    else\n      cmd=\"${pybin}
    pilot2/pilot.py -q ${qarg} -i ${iarg} -j ${jarg} --pilot-user=ATLAS ${pilotargs}\"\n
    \   fi\n  fi\n  echo ${cmd}\n}\n\nfunction get_piloturl() {\n\n  local version=$1\n
    \ local pilotdir=file:///cvmfs/atlas.cern.ch/repo/sw/PandaPilot/tar\n\n  if [[
    -n ${piloturl} ]]; then\n    echo ${piloturl}\n    return 0\n  fi\n\n  if [[ ${version}
    == '1' ]]; then\n    log \"FATAL: pilot version 1 requested, not supported by
    this wrapper\"\n    err \"FATAL: pilot version 1 requested, not supported by this
    wrapper\"\n    apfmon 1\n    sortie 1\n  elif [[ ${version} == '2' ]]; then\n
    \   pilottar=${pilotdir}/pilot2.tar.gz\n  elif [[ ${version} == 'latest' ]]; then\n
    \   pilottar=${pilotdir}/pilot2.tar.gz\n  elif [[ ${version} == 'current' ]];
    then\n    pilottar=${pilotdir}/pilot2.tar.gz\n  else\n    pilottar=${pilotdir}/pilot2-${version}.tar.gz\n
    \ fi\n  echo ${pilottar}\n}\n\nfunction get_pilot() {\n\n  local url=$1\n\n  if
    [[ ${harvesterflag} == 'true' ]] && [[ ${workflowarg} == 'OneToMany' ]]; then\n
    \   cp -v ${HARVESTER_WORK_DIR}/pilot2.tar.gz .\n  fi\n\n  if [[ -f pilot2.tar.gz
    ]]; then\n    tar -xzf pilot2.tar.gz\n    if [ -f pilot2/pilot.py ]; then\n      log
    \"Pilot extracted from existing tarball\"\n      return 0\n    fi\n    log \"FATAL:
    pilot extraction failed\"\n    err \"FATAL: pilot extraction failed\"\n    return
    1\n  fi\n\n  curl --connect-timeout 30 --max-time 180 -sSL ${url} | tar -xzf -\n
    \ if [[ ${PIPESTATUS[0]} -ne 0 ]]; then\n    log \"ERROR: pilot download failed:
    ${url}\"\n    err \"ERROR: pilot download failed: ${url}\"\n    return 1\n  fi\n\n
    \ if [[ -f pilot2/pilot.py ]]; then\n    log \"Pilot download and extraction OK\"\n
    \   return 0\n  else\n    log \"ERROR: pilot extraction failed: ${url}\"\n    err
    \"ERROR: pilot extraction failed: ${url}\"\n    return 1\n  fi\n}\n\nfunction
    muted() {\n  log \"apfmon messages muted\"\n}\n\nfunction apfmon_running() {\n
    \ [[ ${mute} == 'true' ]] && muted && return 0\n  echo -n \"running 0 ${VERSION}
    ${qarg} ${APFFID}:${APFCID}\" > /dev/udp/148.88.67.14/28527\n  out=$(curl -ksS
    --connect-timeout 10 --max-time 20 -d uuid=${UUID} \\\n             -d qarg=${qarg}
    -d state=wrapperrunning -d wrapper=${VERSION} \\\n             -d gtag=${GTAG}
    -d hid=${HARVESTER_ID} -d hwid=${HARVESTER_WORKER_ID} \\\n             ${APFMON}/jobs/${APFFID}:${APFCID})\n
    \ if [[ $? -eq 0 ]]; then\n    log $out\n  else\n    err \"WARNING: wrapper monitor
    ${UUID}\"\n  fi\n}\n\nfunction apfmon_exiting() {\n  [[ ${mute} == 'true' ]] &&
    muted && return 0\n  out=$(curl -ksS --connect-timeout 10 --max-time 20 \\\n             -d
    state=wrapperexiting -d rc=$1 -d uuid=${UUID} \\\n             -d ids=\"${pandaids}\"
    -d duration=$2 \\\n             ${APFMON}/jobs/${APFFID}:${APFCID})\n  if [[ $?
    -eq 0 ]]; then\n    log $out\n  else\n    err \"WARNING: wrapper monitor ${UUID}\"\n
    \ fi\n}\n\nfunction apfmon_fault() {\n  [[ ${mute} == 'true' ]] && muted && return
    0\n\n  out=$(curl -ksS --connect-timeout 10 --max-time 20 \\\n             -d
    state=wrapperfault -d rc=$1 -d uuid=${UUID} \\\n             ${APFMON}/jobs/${APFFID}:${APFCID})\n
    \ if [[ $? -eq 0 ]]; then\n    log $out\n  else\n    err \"WARNING: wrapper monitor
    ${UUID}\"\n  fi\n}\n\nfunction trap_handler() {\n  log \"Caught $1, signalling
    pilot PID: $pilotpid\"\n  kill -s $1 $pilotpid\n  wait\n}\n\nfunction sortie()
    {\n  ec=$1\n  if [[ $ec -eq 0 ]]; then\n    state=wrapperexiting\n  else\n    state=wrapperfault\n
    \ fi\n\n  log \"==== wrapper stdout END ====\"\n  err \"==== wrapper stderr END
    ====\"\n\n  duration=$(( $(date +%s) - ${starttime} ))\n  log \"${state} ec=$ec,
    duration=${duration}\"\n  \n  if [[ ${mute} == 'true' ]]; then\n    muted\n  else\n
    \   echo -n \"${state} ${duration} ${VERSION} ${qarg} ${APFFID}:${APFCID}\" >
    /dev/udp/148.88.67.14/28527\n  fi\n\n  exit $ec\n}\n\n\nfunction main() {\n  #\n
    \ # Fail early, fail often^W with useful diagnostics\n  #\n\n  echo \"This is
    ATLAS pilot2 wrapper version: $VERSION\"\n  echo \"Please send development requests
    to p.love@lancaster.ac.uk\"\n\n  log \"==== wrapper stdout BEGIN ====\"\n  err
    \"==== wrapper stderr BEGIN ====\"\n  UUID=$(cat /proc/sys/kernel/random/uuid)\n
    \ apfmon_running\n  echo\n\n  echo \"---- Host details ----\"\n  echo \"hostname:\"
    $(hostname -f)\n  echo \"pwd:\" $(pwd)\n  echo \"whoami:\" $(whoami)\n  echo \"id:\"
    $(id)\n  echo \"getopt:\" $(getopt -V 2>/dev/null)\n  if [[ -r /proc/version ]];
    then\n    echo \"/proc/version:\" $(cat /proc/version)\n  fi\n  echo \"lsb_release:\"
    $(lsb_release -d 2>/dev/null)\n  \n  myargs=$@\n  echo \"wrapper call: $0 $myargs\"\n\n
    \ cpuinfo_flags=\"flags: EMPTY\"\n  if [ -f /proc/cpuinfo ]; then\n    cpuinfo_flags=\"$(grep
    '^flags' /proc/cpuinfo 2>/dev/null | sort -u 2>/dev/null)\"\n    if [ -z \"${cpuinfo_flags}\"
    ]; then \n      cpuinfo_flags=\"flags: EMPTY\"\n    fi\n  else\n    cpuinfo_flags=\"flags:
    EMPTY\"\n  fi\n  \n  echo \"Flags from /proc/cpuinfo:\"\n  echo ${cpuinfo_flags}\n
    \ echo\n\n  \n  echo \"---- Enter workdir ----\"\n  workdir=$(get_workdir)\n  if
    [[ -f pandaJobData.out ]]; then\n    log \"Copying job description to working
    dir\"\n    cp pandaJobData.out $workdir/pandaJobData.out\n  fi\n  log \"cd ${workdir}\"\n
    \ cd ${workdir}\n  if [[ ${harvesterflag} == 'true' ]]; then\n        export HARVESTER_PILOT_WORKDIR=${workdir}\n
    \       log \"Define HARVESTER_PILOT_WORKDIR : ${HARVESTER_PILOT_WORKDIR}\"\n
    \ fi\n  echo\n  \n  echo \"---- Retrieve pilot code ----\"\n  url=$(get_piloturl
    ${pilotversion})\n  log \"Using piloturl: ${url}\"\n\n  get_pilot ${url}\n  if
    [[ $? -ne 0 ]]; then\n    log \"FATAL: failed to get pilot code\"\n    err \"FATAL:
    failed to get pilot code\"\n    apfmon_fault 1\n    sortie 1\n  fi\n  echo\n  \n
    \ echo \"---- Initial environment ----\"\n  printenv | sort\n  if [[ ${containerflag}
    == 'true' ]]; then\n    log 'Skipping defining VO_ATLAS_SW_DIR due to --container
    flag'\n    log 'Skipping defining ATLAS_LOCAL_ROOT_BASE due to --container flag'\n
    \ else\n    export VO_ATLAS_SW_DIR='/cvmfs/atlas.cern.ch/repo/sw'\n    export
    ATLAS_LOCAL_ROOT_BASE='/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase'\n  fi\n  echo\n
    \ \n  echo \"---- Shell process limits ----\"\n  ulimit -a\n  echo\n  \n  echo
    \"---- Check python version ----\"\n  check_python\n  echo\n\n  echo \"---- Check
    cvmfs area ----\"\n  if [[ ${containerflag} == 'true' ]]; then\n    log 'Skipping
    Check cvmfs area due to --container flag'\n  else\n    check_cvmfs\n  fi\n  echo\n\n
    \ echo \"---- Setup ALRB ----\"\n  if [[ ${containerflag} == 'true' ]]; then\n
    \   log 'Skipping Setup ALRB due to --container flag'\n  else\n    setup_alrb\n
    \ fi\n  echo\n\n  echo \"---- Setup local ATLAS ----\"\n  if [[ ${containerflag}
    == 'true' ]]; then\n    log 'Skipping Setup local ATLAS due to --container flag'\n
    \ else\n    setup_local\n  fi\n  echo\n\n  if [[ ${harvesterflag} == 'true' ]];
    then\n    echo \"---- Create symlinks to input data ----\"\n    log 'Create to
    symlinks to input data from harvester info'\n    setup_harvester_symlinks\n    echo\n
    \ fi\n    \n  if [[ \"${shoalflag}\" == 'true' ]]; then\n    echo \"--- Setup
    shoal ---\"\n    setup_shoal\n    echo\n  fi\n\n  echo \"---- Proxy Information
    ----\"\n  if [[ ${tflag} == 'true' ]]; then\n    log 'Skipping proxy checks due
    to -t flag'\n  else\n    check_proxy\n  fi\n  echo\n  \n  echo \"---- JOB Environment
    ----\"\n  printenv | sort\n  echo\n\n  echo \"---- Build pilot cmd ----\"\n  cmd=$(pilot_cmd)\n
    \ echo cmd: ${cmd}\n  echo\n\n  echo \"---- Ready to run pilot ----\"\n  trap
    trap_handler SIGTERM SIGQUIT SIGSEGV SIGXCPU SIGUSR1 SIGBUS\n  echo\n\n  log \"====
    pilot stdout BEGIN ====\"\n  $cmd &\n  pilotpid=$!\n  log \"pilotpid: $pilotpid\"\n
    \ wait $pilotpid\n  pilotrc=$?\n  log \"==== pilot stdout END ====\"\n  log \"====
    wrapper stdout RESUME ====\"\n  log \"Pilot exit status: $pilotrc\"\n  \n  log
    \"pandaIDs.out files:\"\n  find ${workdir}/pilot2 -name pandaIDs.out -exec ls
    -l {} \\;\n  # note max 30 pandaids for safety\n  pandaids=$(find ${workdir}/pilot2
    -name pandaIDs.out -exec cat {} \\; | xargs echo | cut -d' ' -f-30)\n  log \"pandaids:
    ${pandaids}\"\n  echo\n\n  duration=$(( $(date +%s) - ${starttime} ))\n  apfmon_exiting
    ${pilotrc} ${duration}\n  \n\n  if [[ ${piloturl} != 'local' ]]; then\n      log
    \"cleanup: rm -rf $workdir\"\n      rm -fr $workdir\n  else \n      log \"Test
    setup, not cleaning\"\n  fi\n\n  sortie 0\n}\n\nfunction usage () {\n  echo \"Usage:
    $0 -q <queue> -r <resource> -s <site> [<pilot_args>]\"\n  echo\n  echo \"  --container
    (Standalone container), file to source for release setup \"\n  echo \"  --harvester
    (Harvester at HPC edge), NodeID from HPC batch system \"\n  echo \"  -i,   pilot
    type, default PR\"\n  echo \"  -j,   job type prodsourcelabel, default 'managed'\"\n
    \ echo \"  -q,   panda queue\"\n  echo \"  -r,   panda resource\"\n  echo \"  -s,
    \  sitename for local setup\"\n  echo \"  --piloturl, URL of pilot code tarball\"\n
    \ echo \"  --pilotversion, request particular pilot version\"\n  echo\n  exit
    1\n}\n\nstarttime=$(date +%s)\n\n# wrapper args are explicit if used in the wrapper\n#
    additional pilot2 args are passed as extra args\ncontainerflag='false'\ncontainerarg=''\nharvesterflag='false'\nharvesterarg=''\nworkflowarg=''\niarg='PR'\njarg='managed'\nqarg=''\nrarg=''\nshoalflag=false\ntflag='false'\npiloturl=''\npilotversion='latest'\nmute='false'\nmyargs=\"$@\"\n\nPOSITIONAL=()\nwhile
    [[ $# -gt 0 ]]\ndo\nkey=\"$1\"\ncase $key in\n    -h|--help)\n    usage\n    shift\n
    \   shift\n    ;;\n    --container)\n    containerflag='true'\n    #containerarg=\"$2\"\n
    \   #shift\n    shift\n    ;;\n    --harvester)\n    harvesterflag='true'\n    harvesterarg=\"$2\"\n
    \   mute='true'\n    piloturl='local'\n    shift\n    shift\n    ;;\n    --harvester_workflow)\n
    \   harvesterflag='true'\n    workflowarg=\"$2\"\n    shift\n    shift\n    ;;\n
    \   --mute)\n    mute='true'\n    shift\n    ;;\n    --pilotversion)\n    pilotversion=\"$2\"\n
    \   shift\n    shift\n    ;;\n    --piloturl)\n    piloturl=\"$2\"\n    shift\n
    \   shift\n    ;;\n    -i)\n    iarg=\"$2\"\n    shift\n    shift\n    ;;\n    -j)\n
    \   jarg=\"$2\"\n    shift\n    shift\n    ;;\n    -q)\n    qarg=\"$2\"\n    shift\n
    \   shift\n    ;;\n    -r)\n    rarg=\"$2\"\n    shift\n    shift\n    ;;\n    -s)\n
    \   sarg=\"$2\"\n    shift\n    shift\n    ;;\n    -S|--shoal)\n    shoalflag=true\n
    \   shift\n    ;;\n    -t)\n    tflag='true'\n    POSITIONAL+=(\"$1\") # save
    it in an array for later\n    shift\n    ;;\n    *)\n    POSITIONAL+=(\"$1\")
    # save it in an array for later\n    shift\n    ;;\nesac\ndone\nset -- \"${POSITIONAL[@]}\"
    # restore positional parameters\n\nif [ -z \"${qarg}\" ]; then usage; exit 1;
    fi\n\npilotargs=\"$@\"\n\nfabricmon=\"http://fabricmon.cern.ch/api\"\nfabricmon=\"http://apfmon.lancs.ac.uk/api\"\nif
    [ -z ${APFMON} ]; then\n  APFMON=${fabricmon}\nfi\nmain \"$myargs\"\n"
  templateFile.sdf: |+
    executable = {executableFile}
    arguments = "-s {computingSite} -r {computingSite} -q {pandaQueueName} -j {prodSourceLabel} -i {pilotType} -w generic --pilot-user ATLAS --url https://pandaserver.cern.ch -d --harvester-submit-mode PULL --allow-same-user=False --job-type={jobType} --resource-type SCORE --pilotversion {pilotVersion} {pilotUrlOption}"
    initialdir = {accessPoint}
    universe = grid
    log = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).log
    output = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).out
    error = {logDir}/{logSubdir}/grid.$(Cluster).$(Process).err
    transfer_executable = True
    x509userproxy = {x509UserProxy}
    environment = "PANDA_JSID=harvester-{harvesterID} HARVESTER_ID={harvesterID} HARVESTER_WORKER_ID={workerID} APFMON=http://apfmon.lancs.ac.uk/api APFFID={harvesterID} APFCID=$(Cluster).$(Process)"
    +harvesterID = "{harvesterID}"
    +harvesterWorkerID = "{workerID}"

    grid_resource = nordugrid {ceHostname}
    nordugrid_rsl = (queue = {ceQueueName})(runtimeenvironment = APPS/HEP/ATLAS-SITE-LCG)(runtimeenvironment = ENV/PROXY)(jobname = arc_pilot)(count = 1)(countpernode = 1)(memory = 2000)(walltime = {requestWalltime})(cputime = {requestCputime})(environment = (PANDA_JSID harvester-{harvesterID})(HARVESTER_ID {harvesterID})(HARVESTER_WORKER_ID {workerID})(GTAG {gtag})(APFMON http://apfmon.lancs.ac.uk/api)(APFFID {harvesterID})(APFCID $(Cluster).$(Process)))

    +remote_jobuniverse = 5
    +remote_requirements = True
    +remote_ShouldTransferFiles = "YES"
    +remote_WhenToTransferOutput = "ON_EXIT"
    +remote_TransferOutput = ""

    #+remote_RequestCpus = {nCoreTotal}
    #+remote_RequestMemory = {requestRam}
    #+remote_RequestDisk = {requestDisk}
    #+remote_JobMaxVacateTime = {requestWalltime}
    +ioIntensity = {ioIntensity}

    #+remote_Requirements = JobRunCount == 0
    periodic_remove = (JobStatus == 2 && (CurrentTime - EnteredCurrentStatus) > 604800)
    #+remote_PeriodicHold = ( JobStatus==1 && gridjobstatus=?=UNDEFINED && CurrentTime-EnteredCurrentStatus>3600 ) || ( (JobRunCount =!= UNDEFINED && JobRunCount > 0) ) || ( JobStatus == 2 && CurrentTime-EnteredCurrentStatus>604800 )
    +remote_PeriodicRemove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)

    +sdfPath = "{sdfPath}"

    queue 1

kind: ConfigMap
metadata:
  creationTimestamp: null
  name: condor-file
